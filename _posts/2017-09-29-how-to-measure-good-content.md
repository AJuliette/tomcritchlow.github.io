---
layout: blog
title: How do you measure good content
subtitle: In search of a framework or process to judge editorial... quality?
---

I'm involved in a few projects right now where the quality of the content is in question. It gets to a very simple question but with a complicated answer:

How do you measure content quality?

This is an important question for all kinds of companies. Whether you're a media company producing content or a brand using content as marketing it's important to have an understanding of whether the content you're producing is "good".

Yes, there are some specific benchmarks you need to hit - compelling headlines, spelling, formatting etc. But beyond that - how do you know if what you're writing is... interesting? If it's resonating with your audience?

(And no, this isn't a conversation about happyclappy Medium.)

I'm not looking to reduce everything to numbers here. I'm just looking for some useful frameworks or mental models to guide a team that is producing content to produce better content.

# Defining an editorial mission is a good start...

In my presentation from earlier this year [The State of Content](http://tomcritchlow.com/2017/02/24/the-state-of-media/) at the end I made the link between defining an editorial mission and hiring a strong editor in chief to champion and bring that to life. I included this quote from the [NYT 2020 report](https://www.nytimes.com/projects/2020-report/):

> Our most successful forays into digital journalism [...] have depended on distinct visions established by their leaders — visions supported and shaped by the masthead, and enthusiastically shared by the members of the department. [...] These departments with clear, widely understood missions remain unusual. Most Times journalists cannot describe the vision or mission of their desks.

I love this, and I still think this is one of the best ways to produce strong content that connects with an audience.

But... I'm left feeling slightly unsatisfied. And, in particular, as a consultant I'm often in the weeds with a client trying to help them define an editorial mission and build processes and teams that can stick to it.

And... I don't have a good framework to turn to that helps a team to keep on track? Do you have one?

# Defining an audience and understanding their needs is also good...

I've seen several examples of teams that tried to define an editorial mission without properly defining an audience. And every time they came up short. Also from my state of content presentation, I love this [quote (actually a twitter thread) from Kyle Monson](https://twitter.com/kmonson/status/710129384494071808):

> Whether your employer is a publisher, a brand, or an agency. If you’re making garbage, help them do better. The good ones will listen to you and incorporate your expertise. In my experience, most of them really do want to do it right.

> And believe it or not, super engaged communities often want to hear from the brands who serve them. And super engaged brands want to provide communities with things other than just “content”.

> So if you find yourself writing pablum, narrow the audience to a core group of smart people who care about what the brand has to say. And then convince the brand to say something meaningful to that small, smart audience. Godspeed.

Of course, don't get me wrong... this is hard! Especially the part about narrowing your focus.

# Perhaps interestingness as a measure can guide us?

When I put out a call on Twitter my friend Toby Shorin linked me to this Ribbonfarm piece [books are fake](https://www.ribbonfarm.com/2017/06/01/why-books-are-fake/) and this quote in particular:

> Interestingness is not a fixed property, but a move in a conversation between what everyone in the audience already knows (the “assumption ground“) and the surprise reveal. Being interesting means that the audience shares, or can be made to share, the common knowledge that the author seeks to undermine. Interestingness is a function of whatever body of knowledge is already assumed to be true. Therefore, it can be difficult to see the interestingness – the point – of a fragment of an alien conversation.

Hmm. This sounds useful - can we extend this into a measure of interestingness? Perhaps by shining a light on and agreeing upon a definition or outline of the assumption ground?

I think there's a thread to be explored here. My friend [Sean Blanda](https://twitter.com/SeanBlanda) said:

![](/images/seanblandatweet.png)

I like this but it's still a long way from being a real framework or process you can follow with any rigor... In a loooooong chat conversation between myself, Sean and [Brian Dell](https://twitter.com/itsbdell) we discussed whether it's possible to add any rigor here. I like this from Brian:

![](/images/briandelltweet.png)

I love this! But, it's that soft space. How do you make it less soft? How do you poke it?

I don't think we came to any conclusions. Maybe it's not possible to create metrics here but we must be able to use some kind of process...

# Surveys & Feedback

One tool in my arsenal that I'm relying on is a variation of the [Google Panda Questionnaire](https://www.distilled.net/blog/seo/replicate-googles-panda-questionnaire-processing/). In short, it's a 10-question survey that you can run on any web-page with real humans to get a measure of "quality".

It's useful for a few key reasons:

1) The questions came originally from Google's quality rater guidelines doc so the framework has some relevance. Companies understand why you want to use it as a benchmark.

2) It quantifies otherwise subjective measures like how much users trust the website.

The results from this kind of survey can be very powerful for company execs. This is an example from a client:

![](/images/panda.png)

In my research, I have a hunch that Google based these questions on elements of the System Usability Scale (SUS) originally developed in 1986:

![](/images/sus.png)

[More details on the SUS here](https://www.usability.gov/how-to-and-tools/methods/system-usability-scale.html)

Both the Panda survey and the SUS survey are useful tools but they're much more useful and relevant for evaluating site design and UX, though they do certainly touch on content.

# Maybe there's a version of the Panda survey for evaluating content interestingness?

What would a version of the panda survey look like specifically around judging editorial content? Could we construct an Editorial Value Survey (EVS for short) that looked like the following questions:

- Is this site written by experts
- Did you find value in reading this content?
- Would you have finished reading this piece of content if you were reading by yourself?
- Would you bookmark this content to come back to later?
- Would you share this piece of content
- Would you describe this content as [insert Q relating to our editorial mission see above]
- Would this piece of content make you want to learn more about who wrote it?
- Did you 

# Or, perhaps we need more of an internal process or framework?

My solution here doesn't necessarily need to be a user survey. It could just be an internal process of getting a group of employees and/or writers together and creating a scoring methodology for content. I had one client who could never define good content, so instead created a benchmark for **good enough** content. By defining the worse possible thing they could still publish it actually gave them a clearer focus for evaluating content.

An internal framework could be a short list of questions to interrogate our content and we can have a small team internally rate content on the scale. This could be questions like:

- Did this content meet the intent of the page?
- Is this content meaningful to our audience?
- Does this content say something original to the audience?
- 