---
layout: blog
title: How to measure good content
subtitle: In search of a framework or process to judge editorial quality
---

I'm involved in a few projects right now where the quality of the content is in question. It gets to a very simple question but with a complicated answer:

How do you measure content quality?

This is an important question for all kinds of companies. From Quartz to Intercom and everyone in-between it's important to have an understanding of whether the content you're producing is "good".

Yes, there are some specific benchmarks you need to hit - compelling headlines, spelling, formatting etc. But beyond that - how do you know if what you're writing is... interesting? If it's resonating with your audience?

(And no, this isn't a conversation about happyclappy Medium.)

I'm not looking to reduce everything to numbers here. I'm just looking for some useful frameworks or mental models to guide a team that is producing content to produce better content.

# Defining an editorial mission is a good start...

In my presentation from earlier this year [The State of Content](http://tomcritchlow.com/2017/02/24/the-state-of-media/) at the end I made the link between defining an editorial mission and hiring a strong editor in chief to champion and bring that to life. I included this quote from the [NYT 2020 report](https://www.nytimes.com/projects/2020-report/):

> Our most successful forays into digital journalism [...] have depended on distinct visions established by their leaders — visions supported and shaped by the masthead, and enthusiastically shared by the members of the department. [...] These departments with clear, widely understood missions remain unusual. Most Times journalists cannot describe the vision or mission of their desks.

I love this, and I still think this is one of the best ways to produce strong content that connects with an audience.

But... I'm left feeling slightly unsatisfied. And, in particular, as a consultant I'm often in the weeds with a client trying to help them define an editorial mission and build processes and teams that can stick to it.

And... I don't have a good framework to turn to that helps a team to keep on track? Do you have one?

# Perhaps interestingness as a measure can guide us?

When I put out a call on Twitter my friend Toby Shorin linked me to this Ribbonfarm piece [books are fake](https://www.ribbonfarm.com/2017/06/01/why-books-are-fake/) and this quote in particular:

> Interestingness is not a fixed property, but a move in a conversation between what everyone in the audience already knows (the “assumption ground“) and the surprise reveal. Being interesting means that the audience shares, or can be made to share, the common knowledge that the author seeks to undermine. Interestingness is a function of whatever body of knowledge is already assumed to be true. Therefore, it can be difficult to see the interestingness – the point – of a fragment of an alien conversation.

Hmm. This sounds useful - can we extend this into a measure of interestingness? Perhaps by shining a light on and agreeing upon a definition or outline of the assumption ground?

# Surveys & Feedback

One tool in my arsenal that I'm relying on is a variation of the [Google Panda Questionnaire](https://www.distilled.net/blog/seo/replicate-googles-panda-questionnaire-processing/). In short, it's a 10-question survey that you can run on any web-page with real humans to get a measure of "quality".

It's useful for a few key reasons:

1) The questions came originally from Google's quality rater guidelines doc so the framework has some relevance. Companies understand why you want to use it as a benchmark.

2) It quantifies otherwise subjective measures like how much users trust the website.

The results from this kind of survey can be very powerful for company execs. This is an example from a client:

![](/images/panda.png)

In my research, I have a hunch that Google based these questions on elements of the System Usability Scale (SUS) originally developed in 1986:

![](/images/sus.png)

[More details on the SUS here](https://www.usability.gov/how-to-and-tools/methods/system-usability-scale.html)

# But what about how GOOD the content is?

Both the Panda survey and the SUS survey are useful tools but they're much more useful and relevant for evaluating site design and UX, though they do certainly touch on content.

What would a version of this look like specifically around judging editorial content?

